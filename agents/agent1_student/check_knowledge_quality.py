#!/usr/bin/env python3
"""
Skrypt do sprawdzania jako≈õci danych w bazie wiedzy Qdrant.
Sprawdza duplikaty, kategoryzacjƒô, statystyki.
"""

import os
from collections import Counter, defaultdict
from pathlib import Path
from qdrant_client import QdrantClient
import hashlib

# Konfiguracja
QDRANT_HOST = os.getenv("QDRANT_HOST", "localhost")
QDRANT_PORT = int(os.getenv("QDRANT_PORT", "6333"))
COLLECTION = "agent1_student"
KNOWLEDGE_DIR = Path("chatbot-baza-wiedzy-nowa")

def analyze_qdrant_collection():
    """Analiza kolekcji w Qdrant."""
    print("=" * 80)
    print("ANALIZA BAZY WIEDZY W QDRANT")
    print("=" * 80)
    
    client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
    
    try:
        # Informacje o kolekcji
        collection_info = client.get_collection(COLLECTION)
        print(f"\nüìä Kolekcja: {COLLECTION}")
        print(f"   Liczba punkt√≥w: {collection_info.points_count}")
        print(f"   Status: {collection_info.status}")
        
        # Pobierz wszystkie punkty
        scroll_result = client.scroll(
            collection_name=COLLECTION,
            limit=1000,
            with_payload=True,
            with_vectors=False
        )
        
        points = scroll_result[0]
        print(f"\nüì¶ Pobrano {len(points)} dokument√≥w z Qdrant")
        
        if not points:
            print("\n‚ö†Ô∏è  Brak dokument√≥w w kolekcji!")
            return
        
        # Analiza kategorii
        categories = [p.payload.get('category', 'BRAK KATEGORII') for p in points]
        category_counts = Counter(categories)
        
        print(f"\nüìÇ Kategorie dokument√≥w:")
        for category, count in category_counts.most_common():
            print(f"   {category}: {count} dokument√≥w")
        
        # Analiza ≈õcie≈ºek plik√≥w
        paths = [p.payload.get('path', 'BRAK ≈öCIE≈ªKI') for p in points]
        path_counts = Counter(paths)
        
        print(f"\nüìÅ ≈πr√≥d≈Ça dokument√≥w (top 10):")
        for path, count in path_counts.most_common(10):
            filename = Path(path).name if path != 'BRAK ≈öCIE≈ªKI' else path
            print(f"   {filename}: {count}x")
        
        # Sprawdzenie duplikat√≥w po tre≈õci
        content_hashes = defaultdict(list)
        for p in points:
            content = p.payload.get('content', '')
            if content:
                content_hash = hashlib.md5(content.encode()).hexdigest()
                content_hashes[content_hash].append({
                    'id': p.id,
                    'path': p.payload.get('path', 'BRAK'),
                    'category': p.payload.get('category', 'BRAK'),
                    'preview': content[:100]
                })
        
        # Znajd≈∫ duplikaty
        duplicates = {h: docs for h, docs in content_hashes.items() if len(docs) > 1}
        
        if duplicates:
            print(f"\n‚ùå DUPLIKATY TRE≈öCI ({len(duplicates)} grup):")
            for idx, (hash_val, docs) in enumerate(duplicates.items(), 1):
                print(f"\n   Duplikat #{idx} ({len(docs)} kopii):")
                for doc in docs:
                    print(f"      - ID: {doc['id']}")
                    print(f"        Kategoria: {doc['category']}")
                    print(f"        Plik: {Path(doc['path']).name}")
                    print(f"        PodglƒÖd: {doc['preview']}...")
        else:
            print(f"\n‚úÖ Brak duplikat√≥w tre≈õci!")
        
        # Analiza d≈Çugo≈õci dokument√≥w
        content_lengths = [len(p.payload.get('content', '')) for p in points]
        avg_length = sum(content_lengths) / len(content_lengths) if content_lengths else 0
        min_length = min(content_lengths) if content_lengths else 0
        max_length = max(content_lengths) if content_lengths else 0
        
        print(f"\nüìè Statystyki d≈Çugo≈õci dokument√≥w:")
        print(f"   ≈örednia d≈Çugo≈õƒá: {avg_length:.0f} znak√≥w")
        print(f"   Min: {min_length} znak√≥w")
        print(f"   Max: {max_length} znak√≥w")
        
        # Dokumenty bardzo kr√≥tkie (podejrzane)
        short_docs = [(p.payload.get('path', 'BRAK'), len(p.payload.get('content', ''))) 
                      for p in points if len(p.payload.get('content', '')) < 50]
        
        if short_docs:
            print(f"\n‚ö†Ô∏è  Bardzo kr√≥tkie dokumenty (< 50 znak√≥w):")
            for path, length in short_docs:
                print(f"   {Path(path).name}: {length} znak√≥w")
        
        # Przyk≈Çadowe dokumenty z ka≈ºdej kategorii
        print(f"\nüìÑ Przyk≈Çadowe dokumenty z ka≈ºdej kategorii:")
        for category in set(categories):
            sample = next((p for p in points if p.payload.get('category') == category), None)
            if sample:
                content_preview = sample.payload.get('content', '')[:200]
                print(f"\n   Kategoria: {category}")
                print(f"   Plik: {Path(sample.payload.get('path', 'BRAK')).name}")
                print(f"   PodglƒÖd: {content_preview}...")
        
    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd podczas analizy Qdrant: {e}")
        import traceback
        traceback.print_exc()


def analyze_filesystem():
    """Analiza plik√≥w w systemie plik√≥w."""
    print("\n" + "=" * 80)
    print("ANALIZA PLIK√ìW W FOLDERZE chatbot-baza-wiedzy-nowa")
    print("=" * 80)
    
    if not KNOWLEDGE_DIR.exists():
        print(f"\n‚ùå Folder {KNOWLEDGE_DIR} nie istnieje!")
        return
    
    # Znajd≈∫ wszystkie pliki .txt
    txt_files = list(KNOWLEDGE_DIR.rglob("*.txt"))
    
    print(f"\nüìÅ Znaleziono {len(txt_files)} plik√≥w .txt")
    
    # Grupuj po kategoriach (subfoldery)
    categories = defaultdict(list)
    for file in txt_files:
        # Kategoria = pierwszy subfolder po chatbot-baza-wiedzy-nowa
        parts = file.relative_to(KNOWLEDGE_DIR).parts
        category = parts[0] if len(parts) > 1 else "ROOT"
        categories[category].append(file)
    
    print(f"\nüìÇ Kategorie w systemie plik√≥w:")
    for category, files in sorted(categories.items()):
        print(f"   {category}: {len(files)} plik√≥w")
        for file in sorted(files):
            print(f"      - {file.name}")
    
    # Sprawd≈∫ duplikaty nazw plik√≥w
    filenames = [f.name for f in txt_files]
    filename_counts = Counter(filenames)
    duplicates = {name: count for name, count in filename_counts.items() if count > 1}
    
    if duplicates:
        print(f"\n‚ùå DUPLIKATY NAZW PLIK√ìW:")
        for name, count in duplicates.items():
            print(f"   {name}: {count}x")
            # Poka≈º pe≈Çne ≈õcie≈ºki
            matching_files = [f for f in txt_files if f.name == name]
            for f in matching_files:
                print(f"      - {f.relative_to(KNOWLEDGE_DIR)}")
    else:
        print(f"\n‚úÖ Brak duplikat√≥w nazw plik√≥w!")
    
    # Sprawd≈∫ duplikaty tre≈õci
    print(f"\nüîç Sprawdzanie duplikat√≥w tre≈õci w plikach...")
    content_hashes = defaultdict(list)
    
    for file in txt_files:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                content = f.read()
                content_hash = hashlib.md5(content.encode()).hexdigest()
                content_hashes[content_hash].append(file)
        except Exception as e:
            print(f"   ‚ö†Ô∏è  B≈ÇƒÖd czytania {file.name}: {e}")
    
    duplicate_contents = {h: files for h, files in content_hashes.items() if len(files) > 1}
    
    if duplicate_contents:
        print(f"\n‚ùå DUPLIKATY TRE≈öCI ({len(duplicate_contents)} grup):")
        for idx, (hash_val, files) in enumerate(duplicate_contents.items(), 1):
            print(f"\n   Duplikat #{idx} ({len(files)} plik√≥w):")
            for file in files:
                print(f"      - {file.relative_to(KNOWLEDGE_DIR)}")
    else:
        print(f"\n‚úÖ Brak duplikat√≥w tre≈õci w plikach!")
    
    # Statystyki rozmiaru plik√≥w
    file_sizes = []
    for file in txt_files:
        try:
            size = file.stat().st_size
            file_sizes.append((file.name, size))
        except Exception as e:
            print(f"   ‚ö†Ô∏è  B≈ÇƒÖd statystyk {file.name}: {e}")
    
    if file_sizes:
        avg_size = sum(s for _, s in file_sizes) / len(file_sizes)
        print(f"\nüìä Statystyki rozmiar√≥w plik√≥w:")
        print(f"   ≈öredni rozmiar: {avg_size:.0f} bajt√≥w")
        print(f"\n   Najmniejsze pliki:")
        for name, size in sorted(file_sizes, key=lambda x: x[1])[:5]:
            print(f"      {name}: {size} bajt√≥w")
        print(f"\n   Najwiƒôksze pliki:")
        for name, size in sorted(file_sizes, key=lambda x: x[1], reverse=True)[:5]:
            print(f"      {name}: {size} bajt√≥w")


if __name__ == "__main__":
    print("\nüîç AUDYT JAKO≈öCI BAZY WIEDZY\n")
    
    # Analiza systemu plik√≥w
    analyze_filesystem()
    
    # Analiza Qdrant
    print("\n")
    analyze_qdrant_collection()
    
    print("\n" + "=" * 80)
    print("‚úÖ ANALIZA ZAKO≈ÉCZONA")
    print("=" * 80 + "\n")
